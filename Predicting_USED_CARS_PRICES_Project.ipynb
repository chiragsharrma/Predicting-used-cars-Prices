{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Predicting_Used_cars_Prices_Project.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN9FgJcC+EM0GlsTsohP0DI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chiragsharrma/chiragsharrma/blob/main/Predicting_USED_CARS_PRICES_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O7GLZx6qn1v3"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "df = pd.read_csv('vehicles.csv')\n",
        "print(df.shape)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "buCpBhStoNO0"
      },
      "source": [
        "df.nunique(axis=0) #tocheckthe number of unique values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XIMwS9RjoSqb"
      },
      "source": [
        "df.describe().apply(lambda s: s.apply(lambda x: format(x, 'f')))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HBg0qWDgoZNX"
      },
      "source": [
        "NULL_val = df.isna().sum()\n",
        "def NULL_filter(na, threshold = .4): #only select variables that passes the threshold\n",
        "    col_pass = []\n",
        "    for i in na.keys():\n",
        "        if na[i]/df.shape[0]<threshold:#used to clean the null value of 40% of the total code that is why 0.4 is used above\n",
        "            col_pass.append(i)\n",
        "    return col_pass\n",
        "df_cleaned = df[NULL_filter(NULL_val)]\n",
        "df_cleaned.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yd1saq1Doaz8"
      },
      "source": [
        "df_cleaned = df_cleaned[df_cleaned['price'].between(999.99, 250000)] # Computing IQR to remove outliers in data,But before removing outliers i changed the price range to make it more realistic\n",
        "Q1 = df_cleaned['price'].quantile(0.25)\n",
        "Q3 = df_cleaned['price'].quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "# Filtering Values between Q1-1.5IQR and Q3+1.5IQR. This is a defined formula.\n",
        "df_filtered = df_cleaned.query('(@Q1 - 1.5 * @IQR) <= price <= (@Q3 + 1.5 * @IQR)')\n",
        "df_filtered.boxplot('price')#PLOTTED THE DATA TO SHOW THE RESULT WITHOUT OUTLIERS"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fm7Z9T0BofeD"
      },
      "source": [
        "df_filtered.describe().apply(lambda s: s.apply(lambda x: format(x, 'f')))#JUST TO SEE THE DATA AGAIN"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P0QwHGMMokK-"
      },
      "source": [
        "# cant be newer than year 2020 and cant be older than year 1900 plus I set the odometer range\n",
        "df_filtered = df_filtered[df_filtered['year'].between(1900, 2020)] # = 140000 + 1.5 * (140000-52379)\n",
        "df_filtered = df_filtered[df_filtered['odometer'].between(0, 271431.5)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UYvejCFNonnB"
      },
      "source": [
        "df_final = df_filtered.copy().drop(['id','url','region_url','image_url','region','description','model','state','paint_color'], axis=1\n",
        "df_final.shape #this is based on intuition and some observation_CHIRAGSHARMA_grootdoot"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a88wKWfFowPN"
      },
      "source": [
        "import matplotlib.pylab as plt\n",
        "import seaborn as sns\n",
        "# calculate correlation matrix to see how many of them increase or decrease each other\n",
        "corr = df_final.corr() # plot the heatmap/a too for visualization\n",
        "sns.heatmap(corr, xticklabels=corr.columns, yticklabels=corr.columns, annot=True, cmap=sns.diverging_palette(220, 20, as_cmap=True))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bWrTHXu6oyF0"
      },
      "source": [
        "df_final = pd.get_dummies(df_final, drop_first=True) #I've started the data modelling\n",
        "print(df_final.columns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fIvIzLngo1TT"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler #why? because I want to remove data anomalities by scaling it.\n",
        "X_head = df_final.iloc[:, df_final.columns != 'price'] #suppose odometer is 200k and year is 2k20 so here even\n",
        "# a slight change in the data will cause the other parameter e.g year ,to go mad!! so i am scaling every column which is not price\n",
        "X = df_final.loc[:, df_final.columns != 'price']\n",
        "y = df_final['price']\n",
        "X = StandardScaler().fit_transform(X)  #in the end I want to keep it realistic thats why i did it e.g normalization"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pxVf_N86o6cB"
      },
      "source": [
        "#Now I am doing the ACTUAL DATA MODEL MAKING TO MAKE THE PREDICTIONS\n",
        "#i AM Using RANDOM_FOREST_ALGORITHM HERE PLUS I'AM USING SCKIT-LEARN LIBRARY HERE\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error as mae\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.25, random_state=0)\n",
        "model = RandomForestRegressor(random_state=1)\n",
        "model.fit(X_train, y_train)\n",
        "pred = model.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kj4g8BUZo9PT"
      },
      "source": [
        "print(mae(y_test, pred)) #CHECKING MEAN ABSOLUTE ERROR\n",
        "print(df_final['price'].mean())model.score(X_test,y_test) #DISPLAYING MEAN PRICE "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jXhVIg8epBMB"
      },
      "source": [
        "feat_Importances = pd.Series(model.feature_importances_, index=X_head.columns)\n",
        "feat_Importances.nlargest(25).plot(kind='barh',figsize=(10,10)) #I am plotting a final graph for feature importance\n",
        "#MY MODEL YIELDED A 90.5% ACCURACY WITH MAE GIVEN AT APPROX $1590 AND MEAN PRICE GIVEN AT APPROX $12600"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DFqMy1jXpF7k"
      },
      "source": [
        "#We measure the importance of a feature by calculating the increase\n",
        "#in the model’s prediction error after permuting the feature.\n",
        "#A feature is “important” if shuffling its values increases the model error,\n",
        "#because in this case the model relied on the feature for the prediction.\n",
        "#A feature is “unimportant” if shuffling its values leaves the model error unchanged, because in this case the model ignored the feature for the prediction\n",
        "#BY CHIRAG SHARMA BTECH_SENIOR_YEAR_GGSIPU_MAIT\n",
        "#UNITED_STATES_OF_AMERICA"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}